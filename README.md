# MachineLearning â€” Regression & Classification on Tabular Data
![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.x-orange)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-yellow)
![License](https://img.shields.io/badge/License-Academic-lightgrey)

A compact, reproducible Semester-6 project with two end-to-end ML tasks:

- **Regression (House Prices)** â€” predict sale price using clean pipelines and a **log-target** strategy with clear diagnostics.  
- **Classification (Telco Churn)** â€” predict whether a customer will churn, using **SMOTENC** / **class weights** and tuned models.

Both tasks use **sklearn pipelines** (impute â†’ encode/scale â†’ model), consistent metrics, and tidy plots.

---

## ğŸ“¦ Repository Structure
MachineLearning/
â”œâ”€ Task 1/ # House Price Regression
â”‚ â””â”€ regression.ipynb
â”œâ”€ Task 2/ # Telco Churn Classification
â”‚ â””â”€ classification.ipynb
â””â”€ README.md

yaml
Copy code
> If datasets arenâ€™t in the repo, use the URLs below and place the CSVs beside each notebook before running.

---

## âš™ï¸ Environment
```bash
# (optional) create & activate a virtual env
python -m venv .venv
# Windows:
.venv\Scripts\activate
# macOS/Linux:
# source .venv/bin/activate

# install essentials
pip install -U pip
pip install numpy pandas scikit-learn imbalanced-learn matplotlib seaborn jupyter

# for the Keras MLP in Task 2 (CPU TF is fine)
pip install tensorflow
```

## ğŸš€ Quickstart
1. Open **Task 1/regression.ipynb** or **Task 2/classification.ipynb**.
2. Run cells top to bottom (ensure the dataset CSVs are next to each notebook).
3. Review printed metrics and generated plots/curves.

## ğŸ“˜ Notebook Details

### Task 1 â€” Regression (House Prices)
**Preprocessing:** `ColumnTransformer`  
- Numeric â†’ median impute (+ scale where needed)  
- Categorical â†’ most-frequent impute + one-hot

**Target:** train on `log(price)`; back-transform predictions for reporting.  
**Models:** `RandomForestRegressor` (tuned) and `MLPRegressor`.  
**Evaluation:** MAE, RMSE, RÂ² with residual checks and RF feature-importance bars.  
**Outcome:** RF is a strong tabular baseline; log-target stabilises error.

### Task 2 â€” Classification (Telco Churn)
**Split:** stratified Train/Val/Test with the same preprocessing logic.  

**Imbalance handling:**  
- RF path â†’ `ImbPipeline(preprocess â†’ SMOTENC â†’ RandomForest)`  
- MLP path â†’ dense one-hot features with **class weights** + **EarlyStopping**

**Tuning:**  
- RF â†’ `RandomizedSearchCV` (StratifiedKFold, F1 scoring)  
- MLP â†’ small grid over width/dropout; early stopping on validation loss

**Evaluation:** Accuracy, Precision, Recall, F1, ROC-AUC, PR-AUC, Confusion Matrix, ROC and PR curves.  

**Headline (report):** tuned RF + SMOTENC â‰ˆ Accuracy ~0.77, Precision 0.55, Recall 0.68, F1 0.61, ROC-AUC 0.84, PR-AUC 0.64.  
MLP often yields higher recall on churners; RF remains more balanced overall.

## ğŸ§© Key Ideas
- Use pipelines to prevent leakage (fit on Train; apply to Val/Test).
- Apply SMOTENC on the training split only; use class weights for the MLP.
- Tune with validation folds and fixed seeds (`random_state=42`).
- Use RF feature importances for quick explainability.

## ğŸ“Š Figures (auto-generated)
- Regression: Actual vs Predicted, Residual/Error plots, RF feature importances.  
- Classification: Confusion Matrix, ROC & PR curves, RF feature importances.

## ğŸ“ License
Academic coursework; add a formal license (e.g., MIT) if you plan to reuse.

## ğŸ™Œ Acknowledgements
pandas, scikit-learn, imbalanced-learn, TensorFlow/Keras, matplotlib, seaborn, Jupyter





